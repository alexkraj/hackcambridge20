{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# SmartHome AI Training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    " We set out to train an machine learning system using data from a house in *Sceaux, France*. 47 months of data capture across the house with specific data on 3 rooms was a large training set for us to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keras in /Users/James/opt/anaconda3/lib/python3.7/site-packages (2.3.1)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in /Users/James/opt/anaconda3/lib/python3.7/site-packages (from keras) (1.1.0)\n",
      "Requirement already satisfied: six>=1.9.0 in /Users/James/opt/anaconda3/lib/python3.7/site-packages (from keras) (1.12.0)\n",
      "Requirement already satisfied: h5py in /Users/James/opt/anaconda3/lib/python3.7/site-packages (from keras) (2.9.0)\n",
      "Requirement already satisfied: numpy>=1.9.1 in /Users/James/opt/anaconda3/lib/python3.7/site-packages (from keras) (1.17.2)\n",
      "Requirement already satisfied: keras-applications>=1.0.6 in /Users/James/opt/anaconda3/lib/python3.7/site-packages (from keras) (1.0.8)\n",
      "Requirement already satisfied: pyyaml in /Users/James/opt/anaconda3/lib/python3.7/site-packages (from keras) (5.1.2)\n",
      "Requirement already satisfied: scipy>=0.14 in /Users/James/opt/anaconda3/lib/python3.7/site-packages (from keras) (1.3.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.filterwarnings(\"ignore\",category=DeprecationWarning)\n",
    "    warnings.filterwarnings(\"ignore\",category=FutureWarning)\n",
    "    warnings.filterwarnings(\"ignore\",category=UserWarning)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "For this machine learning project we used keras with all the basic Python data science packages!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from math import sqrt\n",
    "from numpy import concatenate\n",
    "from matplotlib import pyplot\n",
    "from pandas import read_csv\n",
    "from pandas import DataFrame\n",
    "from pandas import concat\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "To create the dataset needed to train the model, the data was prepared using the following function. It creates arrays of data for the same variable but shifted by a timestep. When processed in the machine learning algrorithm this forces the network to learn how to predict the next timestep from the previous step's data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "\tn_vars = 1 if type(data) is list else data.shape[1]\n",
    "\tdf = DataFrame(data)\n",
    "\tcols, names = list(), list()\n",
    "\t# input sequence (t-n, ... t-1)\n",
    "\tfor i in range(n_in, 0, -1):\n",
    "\t\tcols.append(df.shift(i))\n",
    "\t\tnames += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "\t# forecast sequence (t, t+1, ... t+n)\n",
    "\tfor i in range(0, n_out):\n",
    "\t\tcols.append(df.shift(-i))\n",
    "\t\tif i == 0:\n",
    "\t\t\tnames += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "\t\telse:\n",
    "\t\t\tnames += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "\t# put it all together\n",
    "\tagg = concat(cols, axis=1)\n",
    "\tagg.columns = names\n",
    "\t# drop rows with NaN values\n",
    "\tif dropnan:\n",
    "\t\tagg.dropna(inplace=True)\n",
    "\treturn agg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "To load the dataset we used Pandas to create a dataframe. Pandas can load .csv files directly. A few missing data points were contained in the .csv files marked by a '?'. These were removed using `dataset = dataset.replace({'?':0.0} )`. All data was converted to the same datatype to avoid issues later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/James/opt/anaconda3/lib/python3.7/site-packages/numpy/lib/arraysetops.py:568: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  mask |= (ar1 == a)\n"
     ]
    }
   ],
   "source": [
    "# load dataset\n",
    "dataset = read_csv('new_house.csv', header=0, index_col=0, low_memory=False)\n",
    "dataset = dataset.replace({'?':0.0} )\n",
    "values = dataset.values\n",
    "\n",
    "\n",
    "# ensure all data is float\n",
    "values = values.astype('float32')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We used some of the *Scikit* tools to further prepare the data. A `MinMaxScaler` for all of the values so that they had a conversion to lie between 0 and 1. The Scaler also creates an inverse version of itself at the same time so that we can get back to it at the end. We decided to save time on training and only predict overall energy usage. \n",
    "\n",
    "This was not a simple function of the 3 rooms as there was a lot of hidden data that was not recorded in the study."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      var1(t-1)  var2(t-1)  var3(t-1)   var2(t)   var3(t)   var4(t)   var5(t)  \\\n",
      "1  0.000000e+00   0.379069   0.300719  0.481928  0.313669  0.919260  0.475207   \n",
      "2  9.536743e-07   0.481928   0.313669  0.483186  0.358273  0.917922  0.475207   \n",
      "3  9.536743e-07   0.483186   0.358273  0.484445  0.361151  0.919693  0.475207   \n",
      "4  1.907349e-06   0.484445   0.361151  0.329617  0.379856  0.927326  0.326446   \n",
      "5  1.907349e-06   0.329617   0.379856  0.316490  0.375540  0.924730  0.309917   \n",
      "\n",
      "   var6(t)  var7(t)   var8(t)  \n",
      "1      0.0   0.0125  0.516129  \n",
      "2      0.0   0.0250  0.548387  \n",
      "3      0.0   0.0125  0.548387  \n",
      "4      0.0   0.0125  0.548387  \n",
      "5      0.0   0.0250  0.548387  \n"
     ]
    }
   ],
   "source": [
    "# normalize features\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaled = scaler.fit_transform(values)\n",
    "# frame as supervised learning\n",
    "reframed = series_to_supervised(scaled, 1, 1)\n",
    "reframed.drop(reframed.columns[[3,4,5,6,7,8]], axis=1, inplace=True)\n",
    "# drop columns we don't want to predict\n",
    "print(reframed.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Seperating Training and Testing\n",
    "---\n",
    "The overall dataset is huge so we trained using the first $\\sim 16$ months of data. A test set was then generated for just a few months following this sample. The training data has to be reshaped so that it can be fed into the *Long Short-Term Memory Cells* or **LSTMs** that are used in the network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(614762, 1, 9) (614762,) (122952, 1, 9) (122952,)\n"
     ]
    }
   ],
   "source": [
    "values = reframed.values\n",
    "n_train_hours = int(0.3*len(values))\n",
    "train = values[:n_train_hours, :]\n",
    "test = values[n_train_hours:int(1.2*n_train_hours), :]\n",
    "# split into input and outputs\n",
    "train_X, train_y = train[:, :-1], train[:, -1]\n",
    "test_X, test_y = test[:, :-1], test[:, -1]\n",
    "# reshape input to be 3D [samples, timesteps, features]\n",
    "train_X = train_X.reshape((train_X.shape[0], 1, train_X.shape[1]))\n",
    "test_X = test_X.reshape((test_X.shape[0], 1, test_X.shape[1]))\n",
    "print(train_X.shape, train_y.shape, test_X.shape, test_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Training Model\n",
    "---\n",
    "\n",
    "\n",
    "Here we define the model that is going to be trained. This is a very simple model comprised of **50 LSTM** cells that feed into a single dense layer. This dense layer outputs a single value; the value we want for the next time step. The loss function used is the *Mean Absolute Error*.\n",
    "\n",
    "$mae = |PredictedValue-TrueValue|$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# design network\n",
    "model = Sequential()\n",
    "model.add(LSTM(50, input_shape=(train_X.shape[1], train_X.shape[2])))\n",
    "model.add(Dense(1))\n",
    "model.compile(loss='mae', optimizer='adam')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Callback Systems\n",
    "---\n",
    "\n",
    "We use some of the pre-built Keras callback classes to speed up our training processes and make sure that the model we used is the best that we have made so far.\n",
    "\n",
    "The first callback used is an `EarlyStopping` class. This class halts the training if the validation loss has not improved for a set number of epochs known as the *patience*.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "earlyStopping = EarlyStopping(monitor='val_loss', patience=10, verbose=0, mode='min')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The next class used is a `ModelCheckpoint` class. This saves the models throughout the training run so that we can always pull back to the model that performed the best so far. If `EarlyStopping` activates then this means the model will revert back to the state it was 10 epochs ago. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "mcp_save = ModelCheckpoint('.mdl_wts.hdf5', save_best_only=True, monitor='val_loss', mode='min')\n",
    "reduce_lr_loss = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=7, verbose=0, epsilon=1e-4, mode='min')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Now we begin training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 614762 samples, validate on 122952 samples\n",
      "Epoch 1/50\n",
      " - 66s - loss: 0.1087 - val_loss: 0.1014\n",
      "Epoch 2/50\n",
      " - 60s - loss: 0.1043 - val_loss: 0.0994\n",
      "Epoch 3/50\n",
      " - 66s - loss: 0.1024 - val_loss: 0.0991\n",
      "Epoch 4/50\n",
      " - 69s - loss: 0.1010 - val_loss: 0.0990\n",
      "Epoch 5/50\n",
      " - 62s - loss: 0.0997 - val_loss: 0.0989\n",
      "Epoch 6/50\n",
      " - 62s - loss: 0.0986 - val_loss: 0.0983\n",
      "Epoch 7/50\n"
     ]
    }
   ],
   "source": [
    "# fit network\n",
    "history = model.fit(train_X, train_y, epochs=50, batch_size=72, \n",
    "                    validation_data=(test_X, test_y), verbose=2, shuffle=False,\n",
    "                   callbacks=[earlyStopping, mcp_save, reduce_lr_loss])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Here we plot the training loss and the validation loss over time. As you can see eventually the validation loss is below the training loss. This could be a case of overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# plot history\n",
    "pyplot.plot(history.history['loss'], label='train')\n",
    "pyplot.plot(history.history['val_loss'], label='test')\n",
    "pyplot.legend()\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Testing the Model\n",
    "---\n",
    "\n",
    "Here we test the accuracy of the model by giving it data that it has not been trained on from the rest of the time series. We can see how it continues trends and whether that matches the data that we have. We have to scale back out of our reduced scaling before calculating the *Root Mean Squared Error* metric for accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "yhat = model.predict(test_X)\n",
    "print(yhat.shape)\n",
    "test_X = test_X.reshape((test_X.shape[0], test_X.shape[2]))\n",
    "# invert scaling for forecast\n",
    "inv_yhat = concatenate((yhat, test_X[:, 1:]), axis=1)\n",
    "inv_yhat = scaler.inverse_transform(inv_yhat)\n",
    "inv_yhat = inv_yhat[:,0]\n",
    "# invert scaling for actual\n",
    "test_y = test_y.reshape((len(test_y), 1))\n",
    "inv_y = concatenate((test_y, test_X[:, 1:]), axis=1)\n",
    "inv_y = scaler.inverse_transform(inv_y)\n",
    "inv_y = inv_y[:,0]\n",
    "# calculate RMSE\n",
    "rmse = sqrt(mean_squared_error(inv_y, inv_yhat))\n",
    "print('Test RMSE: %.3f' % rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "pyplot.plot(test_X[:,0], inv_y)\n",
    "pyplot.plot(test_X[:,0], inv_yhat)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
