{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# SmartHome AI Training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    " We set out to train an machine learning system using data from a house in *Sceaux, France*. 47 months of data capture across the house with specific data on 3 rooms was a large training set for us to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "!pip install keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "For this machine learning project we used keras with all the basic Python data science packages!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from math import sqrt\n",
    "from numpy import concatenate\n",
    "from matplotlib import pyplot\n",
    "from pandas import read_csv\n",
    "from pandas import DataFrame\n",
    "from pandas import concat\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "To create the dataset needed to train the data, the data was prepared using the following function. It creates arrays of data for the same variable but shifted by a timestep. When processed in the machine learning algrorithm this forces the network to learn how to predict the next timestep from the previous step's data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "\tn_vars = 1 if type(data) is list else data.shape[1]\n",
    "\tdf = DataFrame(data)\n",
    "\tcols, names = list(), list()\n",
    "\t# input sequence (t-n, ... t-1)\n",
    "\tfor i in range(n_in, 0, -1):\n",
    "\t\tcols.append(df.shift(i))\n",
    "\t\tnames += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "\t# forecast sequence (t, t+1, ... t+n)\n",
    "\tfor i in range(0, n_out):\n",
    "\t\tcols.append(df.shift(-i))\n",
    "\t\tif i == 0:\n",
    "\t\t\tnames += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "\t\telse:\n",
    "\t\t\tnames += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "\t# put it all together\n",
    "\tagg = concat(cols, axis=1)\n",
    "\tagg.columns = names\n",
    "\t# drop rows with NaN values\n",
    "\tif dropnan:\n",
    "\t\tagg.dropna(inplace=True)\n",
    "\treturn agg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "To load the dataset we used Pandas to create a dataframe. Pandas can load .csv files directly. A few missing data points were contained in the .csv files marked by a '?'. These were removed using `dataset = dataset.replace({'?':0.0} )`. All data was converted to the same datatype to avoid issues later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/James/opt/anaconda3/lib/python3.7/site-packages/numpy/lib/arraysetops.py:568: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  mask |= (ar1 == a)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      var1(t-1)  var2(t-1)  var3(t-1)   var2(t)   var3(t)   var4(t)   var5(t)  \\\n",
      "1  0.000000e+00   0.379069   0.300719  0.481928  0.313669  0.919260  0.475207   \n",
      "2  9.536743e-07   0.481928   0.313669  0.483186  0.358273  0.917922  0.475207   \n",
      "3  9.536743e-07   0.483186   0.358273  0.484445  0.361151  0.919693  0.475207   \n",
      "4  1.907349e-06   0.484445   0.361151  0.329617  0.379856  0.927326  0.326446   \n",
      "5  1.907349e-06   0.329617   0.379856  0.316490  0.375540  0.924730  0.309917   \n",
      "\n",
      "   var6(t)  var7(t)   var8(t)  \n",
      "1      0.0   0.0125  0.516129  \n",
      "2      0.0   0.0250  0.548387  \n",
      "3      0.0   0.0125  0.548387  \n",
      "4      0.0   0.0125  0.548387  \n",
      "5      0.0   0.0250  0.548387  \n"
     ]
    }
   ],
   "source": [
    "# load dataset\n",
    "dataset = read_csv('new_house.csv', header=0, index_col=0, low_memory=False)\n",
    "dataset = dataset.replace({'?':0.0} )\n",
    "values = dataset.values\n",
    "\n",
    "\n",
    "# ensure all data is float\n",
    "values = values.astype('float32')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We used some of the *Scikit* tools to further prepare the data. A `MinMaxScaler` for all of the values so that they had a conversion to lie between 0 and 1. The Scaler also creates an inverse version of itself at the same time so that we can get back to it at the end. We decided to save time on training and only predict overall energy usage. \n",
    "\n",
    "This was not a simple function of the 3 rooms as there was a lot of hidden data that was not recorded in the study."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# normalize features\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaled = scaler.fit_transform(values)\n",
    "# frame as supervised learning\n",
    "reframed = series_to_supervised(scaled, 1, 1)\n",
    "reframed.drop(reframed.columns[[3,4,5,6,7,8]], axis=1, inplace=True)\n",
    "# drop columns we don't want to predict\n",
    "print(reframed.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Seperating Training and Testing\n",
    "---\n",
    "The overall dataset is huge so we trained using the first $\\sim 16$ months of data. A test set was then generated for just a few months following this sample. The training data has to be reshaped so that it can be fed into the *Long Short-Term Memory Cells* or **LSTMs** that are used in the network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(614762, 1, 9) (614762,) (122952, 1, 9) (122952,)\n"
     ]
    }
   ],
   "source": [
    "values = reframed.values\n",
    "n_train_hours = int(0.3*len(values))\n",
    "train = values[:n_train_hours, :]\n",
    "test = values[n_train_hours:int(1.2*n_train_hours), :]\n",
    "# split into input and outputs\n",
    "train_X, train_y = train[:, :-1], train[:, -1]\n",
    "test_X, test_y = test[:, :-1], test[:, -1]\n",
    "# reshape input to be 3D [samples, timesteps, features]\n",
    "train_X = train_X.reshape((train_X.shape[0], 1, train_X.shape[1]))\n",
    "test_X = test_X.reshape((test_X.shape[0], 1, test_X.shape[1]))\n",
    "print(train_X.shape, train_y.shape, test_X.shape, test_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Training Model\n",
    "---\n",
    "\n",
    "\n",
    "Here we define the model that is going to be trained. This is a very simple model comprised of **50 LSTM** cells that feed into a single dense layer. This dense layer outputs a single value; the value we want for the next time step. The loss function used is the *Mean Absolute Error*.\n",
    "\n",
    "$mae = |PredictedValue-TrueValue|$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# design network\n",
    "model = Sequential()\n",
    "model.add(LSTM(50, input_shape=(train_X.shape[1], train_X.shape[2])))\n",
    "model.add(Dense(1))\n",
    "model.compile(loss='mae', optimizer='adam')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Callback Systems\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/James/opt/anaconda3/lib/python3.7/site-packages/keras/callbacks/callbacks.py:998: UserWarning: `epsilon` argument is deprecated and will be removed, use `min_delta` instead.\n",
      "  warnings.warn('`epsilon` argument is deprecated and '\n"
     ]
    }
   ],
   "source": [
    "earlyStopping = EarlyStopping(monitor='val_loss', patience=10, verbose=0, mode='min')\n",
    "mcp_save = ModelCheckpoint('.mdl_wts.hdf5', save_best_only=True, monitor='val_loss', mode='min')\n",
    "reduce_lr_loss = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=7, verbose=1, epsilon=1e-4, mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 614762 samples, validate on 122952 samples\n",
      "Epoch 1/50\n",
      " - 67s - loss: 0.1254 - val_loss: 0.1146\n",
      "Epoch 2/50\n",
      " - 64s - loss: 0.1089 - val_loss: 0.1022\n",
      "Epoch 3/50\n",
      " - 63s - loss: 0.1046 - val_loss: 0.0998\n",
      "Epoch 4/50\n",
      " - 65s - loss: 0.1027 - val_loss: 0.0999\n",
      "Epoch 5/50\n",
      " - 80s - loss: 0.1013 - val_loss: 0.0996\n",
      "Epoch 6/50\n",
      " - 97s - loss: 0.1002 - val_loss: 0.0993\n",
      "Epoch 7/50\n",
      " - 65s - loss: 0.0992 - val_loss: 0.0989\n",
      "Epoch 8/50\n",
      " - 76s - loss: 0.0983 - val_loss: 0.0987\n",
      "Epoch 9/50\n",
      " - 77s - loss: 0.0974 - val_loss: 0.0980\n",
      "Epoch 10/50\n",
      " - 74s - loss: 0.0961 - val_loss: 0.0969\n",
      "Epoch 11/50\n",
      " - 68s - loss: 0.0943 - val_loss: 0.0947\n",
      "Epoch 12/50\n",
      " - 75s - loss: 0.0911 - val_loss: 0.0903\n",
      "Epoch 13/50\n",
      " - 77s - loss: 0.0856 - val_loss: 0.0839\n",
      "Epoch 14/50\n",
      " - 89s - loss: 0.0795 - val_loss: 0.0815\n",
      "Epoch 15/50\n",
      " - 72s - loss: 0.0768 - val_loss: 0.0797\n",
      "Epoch 16/50\n",
      " - 75s - loss: 0.0756 - val_loss: 0.0753\n",
      "Epoch 17/50\n",
      " - 84s - loss: 0.0746 - val_loss: 0.0741\n",
      "Epoch 18/50\n",
      " - 70s - loss: 0.0737 - val_loss: 0.0732\n",
      "Epoch 19/50\n",
      " - 85s - loss: 0.0729 - val_loss: 0.0726\n",
      "Epoch 20/50\n",
      " - 80s - loss: 0.0721 - val_loss: 0.0716\n",
      "Epoch 21/50\n",
      " - 70s - loss: 0.0715 - val_loss: 0.0711\n",
      "Epoch 22/50\n",
      " - 80s - loss: 0.0709 - val_loss: 0.0704\n",
      "Epoch 23/50\n",
      " - 69s - loss: 0.0701 - val_loss: 0.0694\n",
      "Epoch 24/50\n",
      " - 72s - loss: 0.0697 - val_loss: 0.0688\n",
      "Epoch 25/50\n",
      " - 83s - loss: 0.0692 - val_loss: 0.0683\n",
      "Epoch 26/50\n",
      " - 79s - loss: 0.0687 - val_loss: 0.0677\n",
      "Epoch 27/50\n",
      " - 76s - loss: 0.0682 - val_loss: 0.0674\n",
      "Epoch 28/50\n",
      " - 68s - loss: 0.0678 - val_loss: 0.0671\n",
      "Epoch 29/50\n",
      " - 68s - loss: 0.0674 - val_loss: 0.0667\n",
      "Epoch 30/50\n",
      " - 71s - loss: 0.0669 - val_loss: 0.0662\n",
      "Epoch 31/50\n",
      " - 68s - loss: 0.0664 - val_loss: 0.0657\n",
      "Epoch 32/50\n",
      " - 75s - loss: 0.0660 - val_loss: 0.0651\n",
      "Epoch 33/50\n",
      " - 81s - loss: 0.0656 - val_loss: 0.0647\n",
      "Epoch 34/50\n",
      " - 77s - loss: 0.0651 - val_loss: 0.0641\n",
      "Epoch 35/50\n",
      " - 68s - loss: 0.0646 - val_loss: 0.0637\n",
      "Epoch 36/50\n",
      " - 73s - loss: 0.0641 - val_loss: 0.0637\n",
      "Epoch 37/50\n",
      " - 73s - loss: 0.0637 - val_loss: 0.0640\n",
      "Epoch 38/50\n",
      " - 90s - loss: 0.0633 - val_loss: 0.0622\n",
      "Epoch 39/50\n",
      " - 59s - loss: 0.0631 - val_loss: 0.0622\n",
      "Epoch 40/50\n",
      " - 86s - loss: 0.0627 - val_loss: 0.0637\n",
      "Epoch 41/50\n",
      " - 85s - loss: 0.0626 - val_loss: 0.0636\n",
      "Epoch 42/50\n",
      " - 74s - loss: 0.0624 - val_loss: 0.0619\n",
      "Epoch 43/50\n",
      " - 83s - loss: 0.0622 - val_loss: 0.0618\n",
      "Epoch 44/50\n",
      " - 77s - loss: 0.0620 - val_loss: 0.0619\n",
      "Epoch 45/50\n"
     ]
    }
   ],
   "source": [
    "# fit network\n",
    "history = model.fit(train_X, train_y, epochs=50, batch_size=72, \n",
    "                    validation_data=(test_X, test_y), verbose=2, shuffle=False,\n",
    "                   callbacks=[earlyStopping, mcp_save, reduce_lr_loss])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot history\n",
    "pyplot.plot(history.history['loss'], label='train')\n",
    "pyplot.plot(history.history['val_loss'], label='test')\n",
    "pyplot.legend()\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat = model.predict(test_X)\n",
    "print(yhat.shape)\n",
    "test_X = test_X.reshape((test_X.shape[0], test_X.shape[2]))\n",
    "# invert scaling for forecast\n",
    "inv_yhat = concatenate((yhat, test_X[:, 1:]), axis=1)\n",
    "inv_yhat = scaler.inverse_transform(inv_yhat)\n",
    "inv_yhat = inv_yhat[:,0]\n",
    "# invert scaling for actual\n",
    "test_y = test_y.reshape((len(test_y), 1))\n",
    "inv_y = concatenate((test_y, test_X[:, 1:]), axis=1)\n",
    "inv_y = scaler.inverse_transform(inv_y)\n",
    "inv_y = inv_y[:,0]\n",
    "# calculate RMSE\n",
    "rmse = sqrt(mean_squared_error(inv_y, inv_yhat))\n",
    "print('Test RMSE: %.3f' % rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyplot.plot(test_X[:,0], inv_y)\n",
    "pyplot.plot(test_X[:,0], inv_yhat)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
